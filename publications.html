<ul> 
  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2403.17091" target="_blank">Offline Reinforcement Learning: Role of State Aggregation and Trajectory Data</a> </strong>
          <br> Zeyu Jia, Alexander Rakhlin, Ayush Sekhari<sup style="color: red;">(α-β)</sup>, and Chen-Yu Wei <span style="color: red;"></span>
          <br> <strong>COLT 2024.</strong> 
      </p>
  </li> 

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2401.09681" target="_blank">Harnessing Density Ratios for Online Reinforcement Learning</a> </strong>
          <br> Philip Amortila, Dylan J. Foster, Nan Jiang, Ayush Sekhari<sup style="color: red;">(α-β)</sup>, and Tengyang Xie  
          <br> <strong>ICLR 2024.</strong>  <font color="red">(Spotlight Presentation)</font>       
      </p>
  </li> 

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2311.08384" target="_blank">Offline Data Enhanced On-Policy Policy Gradient with Provable Guarantees</a> </strong>
          <br> Yifei Zhou*, Ayush Sekhari* (equal contribution), Yuda Song, and Wen Sun
          <br> <strong>ICLR 2024.</strong>
      </p>
  </li>

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/pdf/2310.06113.pdf" target="_blank">When is Agnostic Reinforcement Learning Statistically Tractable?</a> </strong>
          <br> Zeyu Jia, Gene Li, Alexander Rakhlin, Ayush Sekhari<sup style="color: red;">(α-β)</sup>, and Nathan Srebro 
          <br> <strong>NeurIPS 2023.</strong>
      </p>
  </li>

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2307.04998" target="_blank">Selective Sampling and Imitation Learning via Online Regression</a> </strong>
          <br> Ayush Sekhari<sup style="color: red;">(α-β)</sup>, Karthik Sridharan, Wen Sun, and Runzhe Wu 
          <br> <strong>NeurIPS 2023.</strong>
          <br> Short version also appeared at <a href="https://interactive-learning-implicit-feedback.github.io/" target="_blank">Interactive Learning with Implicit Human Feedback workshop</a> at ICML 2023.  
      </p>
  </li>

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2307.12926" target="_blank">Contextual Bandits and Imitation Learning via Preference-Based Active Queries</a> </strong>
          <br> Ayush Sekhari<sup style="color: red;">(α-β)</sup>, Karthik Sridharan, Wen Sun, and Runzhe Wu 
          <br> <strong>NeurIPS 2023.</strong>
          <br> Short version also appeared at <a href="https://interactive-learning-implicit-feedback.github.io/" target="_blank">Interactive Learning with Implicit Human Feedback workshop</a>, and <a href="https://sites.google.com/view/mfpl-icml-2023" target="_blank">The Many Facets of Preference-Based Learning workshop</a> at ICML 2023.  
      </p>
  </li>

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2212.10717" target="_blank">Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks</a> </strong>
          <br> Jimmy Z. Di*, Jack Douglas*, Jayadev Acharya, Gautam Kamath, and Ayush Sekhari 
          <br> <strong>NeurIPS 2023.</strong>
      </p>
  </li>

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2211.14250" target="_blank">Model-Free Reinforcement Learning with the Decision-Estimation Coefficient</a> </strong>
          <br> Dylan J. Foster, Noah Golowich, Jian Qian, Alexander Rakhlin, and Ayush Sekhari<sup style="color: red;">(α-β)</sup> 
          <br> <strong>NeurIPS 2023.</strong> 
      </p>
  </li>

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2306.15744" target="_blank">Ticketed Learning-Unlearning Schemes</a> </strong>
          <br> Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Ayush Sekhari<sup style="color: red;">(α-β)</sup>, and Chiyuan Zhang
          <br> <strong>COLT 2023.</strong> 
          <br> Short version at Symposium on the Foundations of Responsible Computing, FORC 2023.  
      </p>
  </li>

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2206.12081" target="_blank">Computationally Efficient PAC RL in POMDPs with Latent Determinism and Conditional Embeddings</a> </strong>
          <br> Masatoshi Uehara, Ayush Sekhari, Jason D. Lee, Nathan Kallus, and Wen Sun 
          <br> <strong>ICML 2023.</strong> 
      </p>
  </li>

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2210.06718" target="_blank">Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient</a> </strong>
          <br> Yuda Song*, Yifei Zhou*, Ayush Sekhari, J. Andrew Bagnell, Akshay Krishnamurthy, and Wen Sun 
          <br> <strong>ICLR 2023.</strong> 
      </p>
  </li>

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2210.06705" target="_blank">From Gradient Flow on Population Loss to Learning with Stochastic Gradient Descent</a> </strong>
          <br> Satyen Kale, Jason D. Lee, Chris De Sa, Ayush Sekhari<sup style="color: red;">(α-β)</sup>, and Karthik Sridharan. 
          <br> <strong>NeurIPS 2022.</strong>
      </p>
  </li>

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2206.13063" target="_blank">On the Complexity of Adversarial Decision Making</a> </strong>
          <br> Dylan J. Foster, Alexander Rakhlin, Ayush Sekhari<sup style="color: red;">(α-β)</sup>, and Karthik Sridharan 
          <br> <strong>NeurIPS 2022.</strong> <font color="red">(Oral Presentation)</font> 
      </p>
  </li>

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2206.12020" target="_blank">Provably Efficient Reinforcement Learning in Partially Observable Dynamical Systems</a> </strong>
          <br> Masatoshi Uehara, Ayush Sekhari, Jason D. Lee, Nathan Kallus, and Wen Sun 
          <br> <strong>NeurIPS 2022.</strong>
      </p>
  </li> 

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2206.09421" target="_blank">Guarantees for Epsilon-Greedy Reinforcement Learning with Function Approximation</a> </strong>
          <br> Christoph Dann, Yishay Mansour, Mehryar Mohri, Ayush Sekhari<sup style="color: red;">(α-β)</sup>, and Karthik Sridharan 
          <br> <strong>ICML 2022.</strong> Short version at <a href='https://rldm.org/'>RLDM 2022</a> - Reinforcement Learning and Decision Making conference.
      </p>
  </li>

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2107.05074" target="_blank">SGD: The Role of Implicit Regularization, Batch-size and Multiple Epochs</a> </strong>
          <br> Satyen Kale, Ayush Sekhari<sup style="color: red;">(α-β)</sup>, and Karthik Sridharan 
          <br> <strong>NeurIPS 2021.</strong> 
      </p>
  </li> 

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2106.11519" target="_blank">Agnostic Reinforcement Learning with Low-Rank MDPs and Rich Observations</a></strong>
          <br> Christoph Dann, Yishay Mansour, Mehryar Mohri, Ayush Sekhari<sup style="color: red;">(α-β)</sup>, and Karthik Sridharan 
          <br> <strong>NeurIPS 2021.</strong> <font color="red">(Spotlight Presentation)</font>
      </p>
  </li>

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2103.03279" target="_blank">Remember What You Want to Forget: Algorithms for Machine Unlearning</a></strong>
          <br> Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh 
          <br> <strong>NeurIPS 2021.</strong> Short version at <a href='https://tpdp.journalprivacyconfidentiality.org/2021/'>TPDP 2021</a> - Theory and Practice of Differential Privacy.
      </p>
  </li> 

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2106.03243" target="_blank">Neural Active Learning with Performance Guarantees</a></strong>
          <br> Zhilei Wang, Pranjal Awasthi, Christoph Dann, Ayush Sekhari, and Claudio Gentile 
          <br> <strong>NeurIPS 2021.</strong> 
      </p>
  </li> 

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2005.03789" target="_blank">Reinforcement Learning with Feedback Graphs</a></strong>
          <br> Christoph Dann, Yishay Mansour, Mehryar Mohri, Ayush Sekhari<sup style="color: red;">(α-β)</sup>, and Karthik Sridharan 
          <br> <strong>NeurIPS 2020.</strong> Short version at <a href='https://sites.google.com/view/icml2018nonconvex/'>ICML 2020 Theoretical Foundations of RL workshop.</a> 
      </p>
  </li>

  <li> 
      <p> 
          <strong> <a href='https://arxiv.org/abs/2006.13476' target="_blank">Second-Order Information in Non-Convex Stochastic Optimization: Power and Limitations</a></strong>
          <br> Yossi Arjevani, Yair Carmon, John C. Duchi, Dylan J. Foster, Ayush Sekhari<sup style="color: red;">(α-β)</sup>, and Karthik Sridharan 
          <br> <strong>COLT 2020.</strong> Honorable mention for best talk award at <a href="https://www.nyas.org/events/2020/14th-annual-machine-learning-symposium/">NYAS ML symposium 2020</a>.
      </p>
  </li>

  <li> 
      <p> 
          <strong> <a href='https://arxiv.org/abs/1902.04686' target="_blank">The Complexity of Making the Gradient Small in Stochastic Convex Optimization</a></strong>
          <br> Dylan J. Foster, Ayush Sekhari<sup style="color: red;">(α-β)</sup>, Ohad Shamir, Nathan Srebro, Karthik Sridharan, Blake Woodworth  
          <br> <strong>COLT 2019.</strong> <strong> <font color="red">(Best Student Paper Award)</font> </strong>
      </p>
  </li>

  <li> 
      <p> 
          <strong> <a href='https://arxiv.org/abs/1810.11059' target="_blank">Uniform Convergence of Gradients for Non-Convex Learning and Optimization</a></strong>
          <br> Dylan J. Foster, Ayush Sekhari<sup style="color: red;">(α-β)</sup>, and Karthik Sridharan 
          <br> <strong>NeurIPS 2018.</strong> Short version at <a href='https://sites.google.com/view/icml2018nonconvex/' target="_blank">ICML 2018 Nonconvex Optimization workshop.</a>
      </p>
  </li>
</ul>


<h2> Preprints / Paper Currently Under Submission </h2>
<ul> 
  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2407.04264" target="_blank">Langevin Dynamics: A Unified Perspective on Optimization via Lyapunov Potentials</a> </strong>
          <br> August Y. Chen, Ayush Sekhari<sup style="color: red;">(α-β)</sup>, and Karthik Sridharan 
          <br> Preprint arXiv:2407.04264. 
          <br> Preliminary version at OPT for ML 2024 Workshop at NeurIPS 2024.
      </p>
  </li> 

  <li> 
      <p> 
          <strong> The Space Complexity of Learning-Unlearning Schemes </strong> 
          <br> Yeshwanth Cherapanamjeri, Sumegha Garg, Nived Rajaraman, Ayush Sekhari<sup style="color: red;">(α-β)</sup>, and Abhishek Shetty 
      </p>
  </li> 

  <li> 
      <p> 
          <strong> System Aware Unlearning Algorithms: Use Lesser, Forget Faster </strong> 
          <br> Linda Lu, Ayush Sekhari<sup style="color: red;">(α-β)</sup>, and Karthik Sridharan 
          <br>  
      </p>
  </li>

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2406.17216" target="_blank">Machine Unlearning Fails to Remove Data Poisoning Attacks</a> </strong> 
          <br> Martin Pawelczyk, Jimmy Z. Di, Yiwei Lu, Gautam Kamath*, Ayush Sekhari* (equal advisory contribution), and Seth Neel* 
          <br> Preprint arXiv:2406.17216. 
          <br> Preliminary version accepted as <font color="red">spotlight presentation</font> at Generative AI and Law Workshop (GenLaw'24) at ICML 2024.
      </p>
  </li>

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2410.08074" target="_blank">Unstable Unlearning: The Hidden Risk of Concept Resurgence in Diffusion Models</a> </strong> 
          <br> Vinith M. Suriyakumar, Rohan Alur, Ayush Sekhari, Manish Raghavan, and Ashia C. Wilson 
          <br> Preprint arXiv:2410.08074.
      </p>
  </li>

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2406.11810" target="_blank">Computationally Efficient RL under Linear Bellman Completeness for Deterministic Dynamics</a> </strong> 
          <br> Runzhe Wu*, Ayush Sekhari* (equal contribution), Akshay Krishnamurthy, Wen Sun 
          <br> Preprint arXiv:2406.11810.
      </p>
  </li>
</ul>


